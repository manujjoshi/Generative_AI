{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnlxNr9-rB9D"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install pypdf\n",
        "!pip install unstructured\n",
        "!pip install sentence_transformers\n",
        "!pip install pinecone-client\n",
        "!pip install llama-cpp-python\n",
        "!pip install huggingface_hub\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udWx0aMnZv-q"
      },
      "source": [
        "# import all the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrLAHU4NsW96"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "import magic\n",
        "import os\n",
        "import nltk\n",
        "from langchain.document_loaders import PyPDFLoader, OnlinePDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Pinecone, FAISS\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "import pinecone\n",
        "import os\n",
        "from langchain.llms import HuggingFaceHub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m1v80HmZyqt"
      },
      "source": [
        "# load the file and extract the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ey-19dV-scmd"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFLoader(\"/content/platform.pdf\")\n",
        "documents = loader.load()\n",
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbJa14VwZ5ry"
      },
      "source": [
        "# splitting in small chunks/parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYgZqrv6R59n"
      },
      "outputs": [],
      "source": [
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x33MLFM9R56h"
      },
      "outputs": [],
      "source": [
        "docs=text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "ZT-Ibetisxd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQZgoYu5Z9ei"
      },
      "source": [
        "# setting the apis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTxTxJT-R53x"
      },
      "outputs": [],
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', '24270767-c8bd-4cd9-9d95-e58569390d61')\n",
        "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'us-west4-gcp-free')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lx7CVTKaCsJ"
      },
      "source": [
        "# embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp5zN8fjR51R"
      },
      "outputs": [],
      "source": [
        "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QHGWZlHaG96"
      },
      "source": [
        "# load into FAISS vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI5AYW5MR5yj"
      },
      "outputs": [],
      "source": [
        "# Get your docsearch ready\n",
        "docsearch = FAISS.from_documents(docs, embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgYgm3zKaKgG"
      },
      "source": [
        "# find the similarity with k means how many number of parts do you require?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"Explain Optiflow plan\"\n",
        "docs=docsearch.similarity_search(query, k=10)\n",
        "docs"
      ],
      "metadata": {
        "id": "XY-5n_HNbGoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ULqeDm9Tc-O"
      },
      "source": [
        "# LLM using Quantized model because of low computational power"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsbwbpOQTmKQ"
      },
      "outputs": [],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT2irMusTrf5"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUKw6rRTTtyA"
      },
      "outputs": [],
      "source": [
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "# Verbose is required to pass to the callback manager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSMg7BlBadbl"
      },
      "source": [
        "# llm model name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvlxVrI5TSRH"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwEmtDnDTekv"
      },
      "outputs": [],
      "source": [
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
        "n_batch = 256  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "\n",
        "# Loading model,\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path,\n",
        "    max_tokens=256,\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    callback_manager=callback_manager,\n",
        "    n_ctx=1024,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "I4yuwxbItnH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtRe7ipealyI"
      },
      "source": [
        "Create a langchain--- Chain the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeHzn3TETecY"
      },
      "outputs": [],
      "source": [
        "chain=load_qa_chain(llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-DtyF2GUyMY"
      },
      "outputs": [],
      "source": [
        "query=\"Explain the second phase\"\n",
        "docs=docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Engineer"
      ],
      "metadata": {
        "id": "WPA6BmOGf7z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate"
      ],
      "metadata": {
        "id": "w2vkoj4jf68K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Explain the answer in 2-3 sentence.\n",
        "Helpful answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\",\"question\"],\n",
        "    template=custom_prompt_template\n",
        ")"
      ],
      "metadata": {
        "id": "-P5b_AXEh4qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm(\n",
        "    prompt_template.format(\n",
        "        context=docs,\n",
        "        question=\"Explain the Document Automation Solution\"\n",
        "    )\n",
        "))"
      ],
      "metadata": {
        "id": "lwsCm0dLiTnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm(\n",
        "    prompt_template.format(\n",
        "        context=docs,\n",
        "        question=\"Explain the second phase of optiflow\"\n",
        "    )\n",
        "))"
      ],
      "metadata": {
        "id": "2VlAeClcmphf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}